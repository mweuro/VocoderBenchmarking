{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Studia\\.conda\\Lib\\inspect.py:988: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import bigvgan\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from vocos import Vocos\n",
    "import speechbrain as sb\n",
    "from speechbrain.utils.fetching import fetch\n",
    "from speechbrain.utils.data_utils import split_path\n",
    "from speechbrain.lobes.models.FastSpeech2 import mel_spectogram\n",
    "from speechbrain.inference.vocoders import HIFIGAN\n",
    "from speechbrain.inference.vocoders import DiffWaveVocoder\n",
    "from transformers import UnivNetFeatureExtractor, UnivNetModel\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.io.wavfile import write\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetworzenie plików za pomocą modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_from_file(file_path, n_mels, sr=22050):\n",
    "    signal, rate = librosa.load(file_path, sr=sr, mono=True)\n",
    "    signal = torch.FloatTensor(signal)\n",
    "    spectrogram, _ = mel_spectogram(\n",
    "        audio=signal,\n",
    "        sample_rate=22050,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=1024,\n",
    "        f_min=0.0,\n",
    "        f_max=8000.0,\n",
    "        power=1,\n",
    "        normalized=False,\n",
    "        min_max_energy_norm=True,\n",
    "        norm=\"slaney\",\n",
    "        mel_scale=\"slaney\",\n",
    "        compression=True\n",
    "    )\n",
    "\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_process = [str(file) for file in Path('data').rglob('*') if file.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1439"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Studia\\.conda\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "c:\\Studia\\.conda\\Lib\\site-packages\\speechbrain\\utils\\checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "100%|██████████| 1439/1439 [19:17<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas przetwarzania: 0.7942535048810204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-libritts-22050Hz\", savedir=\"pretrained_models/tts-hifigan-libritts-22050Hz\")\n",
    "\n",
    "times = []\n",
    "for file_path in tqdm(files_to_process):\n",
    "    spectrogram = get_mel_from_file(file_path, 80)\n",
    "    \n",
    "    start = time.time()\n",
    "    waveforms = hifi_gan.decode_batch(spectrogram)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "\n",
    "    if not os.path.exists('generated/hifigan/' + file_path[5:13]):\n",
    "        os.makedirs('generated/hifigan/' + file_path[5:13])\n",
    "    torchaudio.save('generated\\\\hifigan\\\\' + file_path[5:], waveforms.squeeze(1), 22050)\n",
    "print('Średni czas przetwarzania:', np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Base class for feature extractors.\"\"\"\n",
    "\n",
    "    def forward(self, audio: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract features from the given audio.\n",
    "\n",
    "        Args:\n",
    "            audio (Tensor): Input audio waveform.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Extracted features of shape (B, C, L), where B is the batch size,\n",
    "                    C denotes output features, and L is the sequence length.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method.\")\n",
    "\n",
    "\n",
    "class MelSpectrogramFeatures(FeatureExtractor):\n",
    "    def __init__(self, sample_rate=24000, n_fft=1024, hop_length=256, n_mels=100, padding=\"center\"):\n",
    "        super().__init__()\n",
    "        if padding not in [\"center\", \"same\"]:\n",
    "            raise ValueError(\"Padding must be 'center' or 'same'.\")\n",
    "        self.padding = padding\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            center=padding == \"center\",\n",
    "            power=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, audio, **kwargs):\n",
    "        if self.padding == \"same\":\n",
    "            pad = self.mel_spec.win_length - self.mel_spec.hop_length\n",
    "            audio = torch.nn.functional.pad(audio, (pad // 2, pad // 2), mode=\"reflect\")\n",
    "        mel = self.mel_spec(audio)\n",
    "        features = safe_log(mel)\n",
    "        return features\n",
    "\n",
    "def safe_log(x: torch.Tensor, clip_val: float = 1e-7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the element-wise logarithm of the input tensor with clipping to avoid near-zero values.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        clip_val (float, optional): Minimum value to clip the input tensor. Defaults to 1e-7.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Element-wise logarithm of the input tensor with clipping applied.\n",
    "    \"\"\"\n",
    "    return torch.log(torch.clip(x, min=clip_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Studia\\.conda\\Lib\\site-packages\\vocos\\pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n",
      "100%|██████████| 1439/1439 [01:51<00:00, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas przetwarzania: 0.07181362976540784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(DEVICE)\n",
    "feature_extractor = MelSpectrogramFeatures(\n",
    "    sample_rate=24000,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=100,\n",
    "    padding=\"center\"\n",
    ")\n",
    "\n",
    "times = []\n",
    "for file_path in tqdm(files_to_process):\n",
    "    resample_transform = torchaudio.transforms.Resample(orig_freq=48000, new_freq=24000)\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 24000:\n",
    "        waveform = resample_transform(waveform)\n",
    "    spectrogram = feature_extractor(waveform).to(DEVICE)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    waveforms = vocos.decode(spectrogram)\n",
    "\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    \n",
    "    if not os.path.exists('generated/vocos/' + file_path[5:13]):\n",
    "        os.makedirs('generated/vocos/' + file_path[5:13])\n",
    "    torchaudio.save('generated\\\\vocos\\\\' + file_path[5:], waveforms.cpu().squeeze(1), 22050)\n",
    "\n",
    "print('Średni czas przetwarzania:', np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigV-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from nvidia/bigvgan_v2_24khz_100band_256x\n",
      "Removing weight norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1439/1439 [06:19<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas przetwarzania: 0.2562854376163973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = bigvgan.BigVGAN.from_pretrained('nvidia/bigvgan_v2_24khz_100band_256x', use_cuda_kernel=False)\n",
    "model.remove_weight_norm()\n",
    "model = model.eval().to(DEVICE)\n",
    "\n",
    "times = []\n",
    "for file_path in tqdm(files_to_process):\n",
    "    spectrogram = get_mel_from_file(file_path, 100, sr=model.h.sampling_rate)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        waveforms = model(spectrogram.unsqueeze(0).to(DEVICE)).squeeze(0).cpu()\n",
    "\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    \n",
    "    if not os.path.exists('generated/bigvgan/' + file_path[5:13]):\n",
    "        os.makedirs('generated/bigvgan/' + file_path[5:13])\n",
    "    torchaudio.save('generated\\\\bigvgan\\\\' + file_path[5:], waveforms.squeeze(1), model.h.sampling_rate)\n",
    "\n",
    "print('Średni czas przetwarzania:', np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiffWave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1439/1439 [3:12:38<00:00,  8.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas przetwarzania: 8.018279543015757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "diffwave = DiffWaveVocoder.from_hparams(source=\"speechbrain/tts-diffwave-ljspeech\", savedir=\"pretrained_models/tts-diffwave-ljspeech\")\n",
    "\n",
    "times = []\n",
    "for file_path in tqdm(files_to_process):\n",
    "    spectrogram = get_mel_from_file(file_path, 80)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    waveforms = diffwave.decode_batch(\n",
    "        spectrogram,\n",
    "        hop_len=256,\n",
    "        fast_sampling=True,\n",
    "        fast_sampling_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    \n",
    "    if not os.path.exists('generated/diffwave/' + file_path[5:13]):\n",
    "        os.makedirs('generated/diffwave/' + file_path[5:13])\n",
    "    torchaudio.save('generated\\\\diffwave\\\\' + file_path[5:], waveforms.squeeze(1), 22050)\n",
    "\n",
    "print('Średni czas przetwarzania:', np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnivNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1439/1439 [06:52<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas przetwarzania: 0.25812796456189185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_id_or_path = \"dg845/univnet-dev\"\n",
    "model = UnivNetModel.from_pretrained(model_id_or_path)\n",
    "feature_extractor = UnivNetFeatureExtractor.from_pretrained(model_id_or_path)\n",
    "\n",
    "times = []\n",
    "for file_path in tqdm(files_to_process):\n",
    "    input_audio, sampling_rate = librosa.load(file_path, sr=feature_extractor.sampling_rate)\n",
    "    inputs = feature_extractor(\n",
    "        input_audio, \n",
    "        sampling_rate=sampling_rate, \n",
    "        pad_end=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        waveforms = model(**inputs)\n",
    "    waveforms = torch.Tensor(feature_extractor.batch_decode(**waveforms))\n",
    "    \n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    \n",
    "    if not os.path.exists('generated/univnet/' + file_path[5:13]):\n",
    "        os.makedirs('generated/univnet/' + file_path[5:13])\n",
    "    torchaudio.save('generated\\\\univnet\\\\' + file_path[5:], waveforms.squeeze(1), feature_extractor.sampling_rate)\n",
    "\n",
    "print('Średni czas przetwarzania:', np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\wikto/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n",
      "100%|██████████| 1439/1439 [3:26:19<00:00,  8.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas przetwarzania: 7.269831891354793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')\n",
    "waveglow = waveglow.to(DEVICE)\n",
    "\n",
    "times = []\n",
    "for file_path in tqdm(files_to_process):\n",
    "    y, sr = librosa.load(file_path, sr=22050, mono=True)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024, hop_length=256, win_length=1024, n_mels=80, fmin=0, fmax=8000)\n",
    "    log_mel_spectrogram = np.log(spectrogram + 1e-6)\n",
    "    mel_input = torch.tensor(log_mel_spectrogram).unsqueeze(0).float().to(DEVICE)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        waveforms = waveglow.infer(mel_input)\n",
    "\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "    \n",
    "    if not os.path.exists('generated/waveglow/' + file_path[5:13]):\n",
    "        os.makedirs('generated/waveglow/' + file_path[5:13])\n",
    "    torchaudio.save('generated\\\\waveglow\\\\' + file_path[5:], waveforms.squeeze(1).cpu(), 22050)\n",
    "\n",
    "print('Średni czas przetwarzania:', np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liczenie metryk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from scripts.metrics import calculate_metrics_for_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bigvgan', 'diffwave', 'hifigan', 'univnet', 'vocos', 'waveglow']\n",
    "src_paths = [str(file) for file in Path('data').rglob('*') if file.is_file()]\n",
    "gen_paths = [[str(file) for file in Path(f'generated/{model}').rglob('*') if file.is_file()] for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [07:30, 75.02s/it][00:00<?, ?it/s]\n",
      "6it [07:16, 72.67s/it][07:55<1:11:18, 475.42s/it]\n",
      "6it [07:35, 75.83s/it][15:34<1:02:05, 465.68s/it]\n",
      "6it [07:20, 73.42s/it][23:34<55:05, 472.26s/it]  \n",
      "6it [07:38, 76.34s/it][31:18<46:54, 469.03s/it]\n",
      "6it [07:26, 74.40s/it][39:22<39:31, 474.39s/it]\n",
      "6it [07:23, 73.90s/it][47:12<31:32, 473.06s/it]\n",
      "6it [07:47, 77.85s/it][55:00<23:34, 471.36s/it]\n",
      "6it [07:44, 77.37s/it][1:03:13<15:56, 478.06s/it]\n",
      "6it [07:32, 75.48s/it][1:11:23<08:01, 481.78s/it]\n",
      "100%|██████████| 10/10 [1:19:20<00:00, 476.06s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics_for_all_data(src_paths, gen_paths, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigvgan</th>\n",
       "      <th>diffwave</th>\n",
       "      <th>hifigan</th>\n",
       "      <th>univnet</th>\n",
       "      <th>vocos</th>\n",
       "      <th>waveglow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sdr</th>\n",
       "      <td>18.742272</td>\n",
       "      <td>14.422735</td>\n",
       "      <td>17.105282</td>\n",
       "      <td>22.180890</td>\n",
       "      <td>14.784264</td>\n",
       "      <td>14.769862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fad</th>\n",
       "      <td>0.346752</td>\n",
       "      <td>1.728151</td>\n",
       "      <td>0.377490</td>\n",
       "      <td>0.273113</td>\n",
       "      <td>0.304175</td>\n",
       "      <td>1.863754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kid</th>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcd</th>\n",
       "      <td>1.088343</td>\n",
       "      <td>1.202381</td>\n",
       "      <td>0.951637</td>\n",
       "      <td>0.834505</td>\n",
       "      <td>1.099907</td>\n",
       "      <td>1.867460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pesq</th>\n",
       "      <td>1.483380</td>\n",
       "      <td>1.841032</td>\n",
       "      <td>2.991883</td>\n",
       "      <td>2.977830</td>\n",
       "      <td>1.184869</td>\n",
       "      <td>1.664157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bigvgan   diffwave    hifigan    univnet      vocos   waveglow\n",
       "sdr   18.742272  14.422735  17.105282  22.180890  14.784264  14.769862\n",
       "fad    0.346752   1.728151   0.377490   0.273113   0.304175   1.863754\n",
       "kid    0.000165   0.003033   0.000230   0.000093   0.000112   0.003188\n",
       "mcd    1.088343   1.202381   0.951637   0.834505   1.099907   1.867460\n",
       "pesq   1.483380   1.841032   2.991883   2.977830   1.184869   1.664157"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
